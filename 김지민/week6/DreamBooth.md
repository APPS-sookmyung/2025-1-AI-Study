# DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation

## 핵심 목표
- 몇 장의 이미지(3~5장)만으로 특정 피사체(subject)를 학습
- 학습한 피사체를 다양한 문맥에서 고품질 이미지로 생성
- 예시: 사용자의 반려견 사진 몇 장으로 "정글 속의 [V] 강아지" 생성

## 주요 기법

### 1. 희귀 토큰 식별자(Rare-token Identifier)
- 기존 어휘에서 드물게 사용되는 토큰을 선택하여 특정 피사체에 할당
- 예: "[V]"와 같은 희귀 토큰을 사용하여 "A [V] dog" 프롬프트 구성
- 모델이 "[V]"를 특정 피사체와 연관지어 학습하게 됨
- T5-XXL tokenizer 어휘 중 5000~10000번대 토큰이 적합(의미 희박)
- 디토크나이즈 결과 길이가 3글자 이하가 이상적

### 2. 클래스별 사전 보존 손실(Class-specific Prior Preservation Loss)
- 모델이 새로운 피사체를 학습하면서 기존 클래스(예: "dog")의 일반적 특성 유지
- 과적합(overfitting)이나 언어 이동(language drift) 현상 방지
- 모델이 스스로 만든 예시로 학습:
  - 파인튜닝 전 모델이 "a dog"의 이미지를 약 1000장 생성
  - 이 이미지들을 학습 데이터에 포함시켜 기존 지식 유지
- 결과:
  - "dog"는 여전히 다양한 일반 개 생성
  - "[V] dog"는 사용자의 특정 강아지 생성
  - 다양한 자세와 표현 가능

### 3. 2단계 파인튜닝 프로세스
- 저해상도 모델 파인튜닝: 입력 이미지와 프롬프트로 저해상도(64×64) 텍스트-이미지 모델 파인튜닝
- 초해상도 모델 파인튜닝: 저해상도와 고해상도 이미지 쌍으로 초해상도 모델 파인튜닝하여 세부 특징 유지

## 기존 기술과의 차별점

### 1. 이미지 합성 기술과 비교
- 기존: 단순 이미지 합성이나 3D 재구성 방식
- 한계: 조명/그림자/접촉 표현 어려움, 새로운 장면 생성 불가
- DreamBooth: 자연스러운 새로운 배경과 자세 생성 가능

### 2. 텍스트 기반 이미지 편집과 비교
- 기존: GAN+CLIP 조합으로 표정이나 색상 변경
- 한계: 정해진 구조(얼굴, 정면 사진 등)에서만 잘 작동
- DreamBooth: 다양한 대상의 일관된 편집 가능

### 3. 텍스트-이미지 생성 모델과 비교
- 기존: Imagen, DALL-E2, Stable Diffusion 등은 텍스트로 이미지 생성
- 한계: 같은 대상의 외형 일관성 유지 어려움
- DreamBooth: 대상의 정체성(identity) 유지하면서 다양한 이미지 생성

## 기술적 배경

### 텍스트-투-이미지 디퓨전 모델 작동 원리
- 처음에는 노이즈(랜덤 점들)만 존재 → N(0,I) (정규분포 샘플)
- 프롬프트(텍스트)를 텍스트 인코더로 변환해 조건 벡터 c = φ(P) 생성
- 디퓨전 모델이 이 조건을 이용해 노이즈를 점점 이미지로 변환
- 반복하며 실제 이미지에 가까운 결과 생성

### 파인튜닝 방식의 중요성
- 단순 토큰 학습이 아닌 모델 자체를 파인튜닝
- 피사체를 모델의 출력 공간에 직접 임베딩
- 결과: 핵심 시각 특징 보존 및 일관된 이미지 생성
