# 📄 논문 리뷰: Adam – A Method for Stochastic Optimization

## **링크**: [arXiv:1412.6980](https://arxiv.org/abs/1412.6980)

## 연구 목적

Adam은 확률적 목적 함수의 **경사 기반 최적화 문제**를 해결하기 위한 **효율적이고 적응적인 1차 최적화 알고리즘**을 제안한다. 기존 알고리즘들의 한계를 보완하면서도 학습률을 파라미터별로 자동 조정하여, 희소한 그래디언트나 노이즈가 있는 환경에서도 잘 작동하도록 설계되었다.

---

## 방법 요약

Adam은 다음 두 가지 아이디어를 결합한다.

- **AdaGrad**: 희소한 특징에 강함 (그래디언트 누적 제곱 기반)
- **RMSProp**: 비정상 목적 함수에 강함 (지수 이동 평균 기반)

### 주요 수식

- 1차 모멘트 (평균) 추정:  
  `m_t = β₁ · m_{t-1} + (1 - β₁) · g_t`
- 2차 모멘트 (분산) 추정:  
  `v_t = β₂ · v_{t-1} + (1 - β₂) · g_t²`
- 편향 보정:  
  `m̂_t = m_t / (1 - β₁ᵗ)`  
  `v̂_t = v_t / (1 - β₂ᵗ)`
- 파라미터 업데이트:  
  `θ_t ← θ_{t-1} - α · m̂_t / (√v̂_t + ε)`

> 이 방식으로 각 파라미터에 맞춘 **적응적 학습률**이 자동 설정

---

## 실험 결과

### 평가 모델

- 다중 클래스 로지스틱 회귀 (MNIST, IMDB)
- 다층 신경망 (MLP, dropout 적용)
- 합성곱 신경망 (CNN – CIFAR-10)

### 비교 대상

- SGD + Nesterov Momentum
- AdaGrad
- RMSProp
- AdaDelta
- SFO (Quasi-Newton method)

### 성능 요약

- Adam은 **빠른 수렴**, **강한 일반화**, **편향 보정의 중요성**에서 우수한 결과
- 희소한 데이터(IMDB BoW)에서 Adagrad 수준의 성능
- CNN에서도 초기엔 Adagrad와 비슷하지만 **장기적으로 더 우수한 성능**

---

## 이론적 기여

- **수렴 이론**: O(√T) regret bound 확보
- **편향 보정**: 초기에 모멘트 추정이 0으로 편향되는 문제를 해결
- **AdaMax**: L∞ 노름 기반 확장도 제시 (더 단순하고 안정적)

---

## 장점

- 적응형 학습률 (parameter-wise)
- 희소/노이즈 그래디언트에서도 안정적
- 하이퍼파라미터 조정이 덜 민감함
- 구현이 간단하고 메모리 효율적
- 비정상 목적 함수에도 적용 가능

---

## 한계 및 고려사항

- β값이 클 경우, **편향 보정이 없으면 발산** 가능 → 반드시 보정 필요
- CNN에서는 2차 모멘트 추정값이 빨리 사라져 `ε`가 주요 역할을 하게 됨

---

## 결론

Adam은 경사 기반 최적화 문제에서 **가장 널리 쓰이는 알고리즘 중 하나**로 실용성, 효율성, 적응성 측면에서 뛰어나며, 현재 대부분의 딥러닝 프레임워크에서 기본 옵티마이저로 사용된다.
