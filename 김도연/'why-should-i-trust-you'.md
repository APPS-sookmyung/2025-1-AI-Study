# 📄 논문 리뷰: "Why Should I Trust You?" Explaining the Predictions of Any Classifier

## **링크**: [arXiv:1602.04938](https://arxiv.org/pdf/1602.04938)

## 연구 요약 및 목적

> **LIME(Local Interpretable Model-agnostic Explanations)**은 어떤 머신러닝 모델이든 로컬 영역에서 해석 가능한 모델로 근사해 예측을 설명할 수 있는 프레임워크
> 사람 중심의 신뢰 가능한 AI를 위해, 예측 결과에 대한 이유를 설명함으로써 사용자의 판단을 돕는 것을 목표로 함

---

## 주요 기여

1. **LIME 알고리즘 제안**

   - 모든 모델(classifier, regressor 포함)에 대해 **모델 비의존적**으로 작동
   - 입력 주변(local)에서 해석 가능한 모델을 학습하여 예측을 설명

2. **SP-LIME(Submodular Pick) 제안**

   - 여러 개의 예측 중 **대표적이고 중복되지 않는 예측을 선택**하여 전체 모델을 설명
   - 서브모듈러 최적화를 통해 정보 다양성 보장

3. **폭넓은 실험 진행**
   - 텍스트 분류(SVM), 이미지 분류(Inception Net) 등 다양한 모델에 적용
   - **비전문가 사용자(일반인)도 설명을 통해** 더 나은 모델을 선택하고, 오류를 발견하며, 신뢰성 판단이 가능함을 입증

---

## 핵심 개념 정리

### LIME의 기본 아이디어

- 원래의 예측 함수 `f(x)`를, 입력 `x` 근처에서 해석 가능한 모델 `g(x')`로 근사함
- 해석 모델은 **로컬 충실성(local fidelity)**을 만족해야 함
- 최적화 문제: ξ(x) = argmin_g L(f, g, π_x) + Ω(g)
- `L(f, g, π_x)`: 원래 모델과 해석 모델의 차이 (로컬 영역에서)
- `π_x`: x 주변에서의 가중치
- `Ω(g)`: 해석 모델 g의 복잡도

---

## 실험 요약

### 1. 예측 설명의 충실도 평가

- LIME은 다른 방법(Parzen, Random, Greedy)보다 훨씬 높은 정확도로 예측의 핵심 feature를 복원
- 특히 텍스트와 이미지 분류 문제에서 90% 이상의 feature 재현률

### 2. 예측 신뢰도 판단 (Should I trust this prediction?)

- 일부 feature를 “신뢰할 수 없음”으로 간주하여, 설명 내 포함 여부로 예측 신뢰성을 평가
- LIME 설명을 바탕으로 잘못된 예측을 탐지 가능

### 3. 모델 신뢰도 판단 (Can I trust this model?)

- 유사한 정확도를 가진 두 모델을 비교할 때, 설명을 통해 **일반화 성능이 더 좋은 모델**을 선택할 수 있음

### 4. 비전문가 실험 (Mechanical Turk)

- 설명을 통해 비전문가도 feature engineering을 수행하고 더 신뢰할 수 있는 모델을 학습 가능
- “Husky vs Wolf” 문제에서, 사용자는 눈(snow)이라는 허위 상관관계를 식별

---

## 대표 이미지 예시

### Figure 1: 개별 예측 설명 예시 (텍스트)

- 독감 예측 사례에서 재채기, 두통 등이 예측을 지지하고, 피로 없음이 반대함을 시각화.
- 사용자가 예측 결과에 **신뢰를 가질 수 있게** 도움.

### Figure2: 두 분류기의 텍스트 분류 설명 비교

- 같은 문서에 대해 서로 다른 모델이 어떤 단어에 주목하는지 시각화.
- 예: 어떤 분류기는 "Posting"이라는 단어를 중요한 특징으로 사용 → 실제 클래스와 관련 없음.
- 모델의 **판단 기준이 합리적인지를 평가**할 수 있음.

### Figure7: 이미지 분류기 설명 (Inception model)

- 예: '일렉트릭 기타'로 예측된 이미지를 해석할 때, 기타 본체가 아닌 프렛보드(superpixel)를 중요하게 봄.
- 신뢰할 수 있는 근거인지 **시각적으로 확인 가능**함.

### Figure11: Husky vs Wolf – 잘못된 상관관계 식별

- 눈(snow) 배경이 있는 이미지가 wolf로 분류되는 오류 발생.
- LIME 설명을 통해 모델이 눈 배경에 과도하게 의존하고 있다는 점을 사용자에게 **명확히 시각화**함.

---

## 논문의 시사점

- **모델의 해석 가능성은 신뢰의 핵심 요소**
- 단순 정확도 평가를 넘어, 사용자가 모델의 판단 과정을 이해하고 판단할 수 있어야 함
- 설명은 AI의 **투명성, 신뢰성, 공정성** 확보에 핵심 도구로 작용함

---

## 향후 연구 방향 (Conclusion & Future Work)

- 결정트리 등 다른 해석 모델 확장 적용 가능성
- 이미지, 음성, 의료 분야 등 다양한 도메인 확장
- **실시간 해석 가능한 시스템** 구현 (병렬 처리, GPU 사용 등)
- 다양한 해석 모델군 탐색을 위한 프레임워크 확장 예정

---
