BERT는 트랜스포머 인코더 기반의 양방향 언어 표현 모델이다. 기존 자연어 처리 모델은 개별 태스크별로 별도의 구조와 학습이 필요했으나, 최근에는 사전학습과 미세조정의 두 단계를 차용하는 추세이다. BERT는 MLM과 NSP라는 두 가지 사전학습 태스크를 결합하여 높은 데이터 처리 능력을 보여준다.

- Masked Language Model (MLM): 입력 문장에서 전체 토큰 중 15%를 선택하여 마스킹하고, 해당 위치의 원래 토큰을 예측하는 방식이다. 이를 통해 모델은 모든 층에서 양방향 정보를 학습할 수 있다.
- Next Sentence Prediction (NSP): 두 문장이 연속적으로 등장하는 문장인지 예측하는 이진 분류 태스크이다. 실제 이어지는 문장은 50%, 랜덤 문장은 50%의 비율로 구성되며, 문장 간 관계 이해를 학습한다.

BERT
BERT는 Transformer 인코더 구조만으로 이루어진 모델이다. 트랜스포머는 입력된 단어들 간의 상호작용을 고려하여 각 단어가 다른 단어와 어떤 관계를 맺는지 파악하는 기법인 self-attention 메커니즘을 통해 문장 내 모든 단어 쌍 간의 관계를 모델링하여 단순한 앞이나 뒤의 정보 뿐 아니라 전체 문맥을 양방향으로 반영한 표현을 학습할 수 있다.

BERT의 입력은 세 가지 임베딩(token, segment, position)의 합으로 표현된다.

1. Token Embedding : WordPiece 토크나이저를 통해 분할된 token에 대해 고유한 임베딩 벡터를 생성한다.
2. Segment Embedding : 문장이 여러 개 있는 경우 어떤 token이 어떤 문장에 속하는지를 구분한다.
3. Position Embedding : 각 token이 시퀀스에서 몇 번째 위치에 있는지를 알려준다.

- BERT는 SQuAD v1.1 (질의응답)에서는 사람 수준에 가까운 성능을 보였으며, SQuAD v2.0 (답이 없을 수도 있는 질의응답)과 SWAG (상식적 문장 완성)등의 주요 벤치마크에서 기존 언어모델들의 최고 성능을 뛰어넘는 결과를 보였다. 