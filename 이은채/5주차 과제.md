## Scalable Diffusion Models with Transformers

- 기존의 CNN을 사용한 U-Net 기반 이미지 생성 모델 대신 Transformer 기반 아키텍처(DiT, Diffusion Transformer)를 제안한 논문


## Transformer 구조 및 동작 원리

1. 입력을 패치로 나누기 (Patch Embedding)
- 이미지를 16×16 크기 패치들로 잘라서, 각각을 벡터로 변환 (Flatten + Linear)
- 이렇게 만든 벡터들을 토큰(token)으로 취급

2. 포지셔널 인코딩 추가
- Transformer는 각 토큰의 순서를 모르기 때문에 각 토큰에 위치 정보를 더하는 포지셔널 인코딩을 붙임

3. 트랜스포머 블록 반복
다음의 Transformer block을 반복:
LayerNorm → Multi-Head Self-Attention → LayerNorm → MLP (Feedforward Network)


## 확산 조건 주입 방식 : adaLN (Adaptive LayerNorm)

- Adaptive LayerNorm (adaLN)을 통해 timestep, 클래스 정보 등 조건을 Transformer에 효과적으로 주입하여 생성된 이미지의 품질을 향상시킴
- LayerNorm(x) → scale(t, y) * norm(x) + shift(t, y)
- t: 현재 timestep, y: 클래스 라벨 


## 장점
- 더 많은 토큰과 더 큰 모델 크기를 자연스럽게 처리 가능
- 이미지를 토큰화해서 self-attention을 통해 전역적으로 처리하고, 조건 정보를 LayerNorm 안에서 선택적으로 주입해 훨씬 유연하고 확장성 있음


